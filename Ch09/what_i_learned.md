# What I Learned

## Markov Chains
- k is how many previous (words, letters, numbers, etc) are taken into account in predicting the next (word, letter, number, etc). k=0 means pure randomness. The higher the k the tighter the constraints until, eventually, your program can only produce exactly what is in the training corpus. 
- The book makes the argument that for a haiku we want a k=2

## Python

- the logging module. I must have come across it before now, but I've completely forgotten what usefulness it has and how to use it. 
